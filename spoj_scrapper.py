__author__ = 'sagarpatel'

from bs4 import BeautifulSoup
import urllib.request
from bs4 import NavigableString


def add_newline(string):
    line_length = 150
    new_string = ""
    l = len(string)
    if l < line_length:
        return string
    for ii in range((l//line_length+1)):
        t = (string[ii*line_length:(ii+1)*line_length])
        if "\n" not in t:
            new_string += (t + '\n')
        else:
            new_string += t
    return new_string


def print_text_of_tag(file_out,base_tag):
    if type(base_tag)==None:
        return

    if type(base_tag)!=NavigableString:
        for i in base_tag.children:
            print_text_of_tag(file_out,i)
    else:
        output_string = add_newline(str(base_tag.string))
        file_out.write(output_string)
        if (base_tag.parent.name == "h3"):
            output_string = "\n" + ("-")*len(output_string) + "\n"
            file_out.write(output_string)


def write_to_page(page_url,count):
    try:
        #print("waiting for the page...")
        page = urllib.request.urlopen(page_url)
        #print("got the page...")
        soup = BeautifulSoup(page.read(),"html.parser")

        all_h2 = soup.find_all("h2")
        problem_body_tag = None
        problem_heading = None

        for i in all_h2:
            if i.has_attr("id") and i["id"] == "problem-name":
                problem_heading = str(i.string)
                all_siblings = i.next_siblings
                for i in all_siblings:
                    if i.name == "div":
                        if i.has_attr("id") and i["id"] == "problem-body":
                            problem_body_tag = i
                            break
                break

        # parsing it to a text file.

        problem_code = str(count) + "-" + problem_heading.split()[0] + ".txt"

        file_out = open(problem_code,"w")

        problem_heading += '\n'
        problem_heading += ('*') * (len(problem_heading)-1)
        problem_heading += '\n'
        file_out.write(problem_heading)

        print_text_of_tag(file_out, problem_body_tag)

        file_out.write("*" * 49)
        file_out.write("\n\n" + "generated by easy_spoj_scrapper.py(by sagarpatel)")

        print(problem_code, " Written Successfully :)")
    except Exception as e:
        print(str(e))
        return

main_page = urllib.request.urlopen("http://www.spoj.com/problems/classical/sort=6")
main_soup = BeautifulSoup(main_page,"html.parser")
a_all = main_soup.find_all("a")
fixed_str = "http://www.spoj.com/problems/"
i = 0
url_list=[]

for a in a_all:
    if ("problems" in a["href"] and "classical" not in a["href"]):
        i += 1
        if i < 6:
            continue
        problem_code = a["href"].split("/")[-2]
        if problem_code!='':
            final_str = fixed_str + problem_code + "/"
            url_list.append(final_str)

count = 1
for url in url_list:
    write_to_page(url,count)
    count += 1
